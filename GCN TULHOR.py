# -*- coding: utf-8 -*-
"""Copy of BERT-Simple.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cMxkMx4Z5XLhYIsjfTEHWkZjBmZCbZCh
"""
import os
devices = "1,2,3,4,5,6,7,8,9"
os.environ['CUDA_VISIBLE_DEVICES'] = devices
cvd = os.getenv('CUDA_VISIBLE_DEVICES')
import random
import typing
from collections import Counter
from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd
import torch
import pickle
from tqdm import tqdm
from torch.utils.data import Dataset
from torchtext.vocab import vocab
from torchtext.data.utils import get_tokenizer
from gensim.models import Word2Vec
import h3
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
import warnings
from balanced_loss import Loss
warnings.simplefilter(action='ignore')

class BERT_SIMPLE_PREP(Dataset):
    CLS = '[CLS]'
    PAD = '[PAD]'
    MASK = '[MASK]'

    MASK_PERCENTAGE = 0.4

    MASKED_INDICES_COLUMN = 'masked_indices'
    TARGET_COLUMN = 'indices'
    TOKEN_MASK_COLUMN = 'token_mask'
    LABEL_COLUMN = "user_id"

    OPTIMAL_LENGTH_PERCENTILE = 100

    def __init__(self, ds, should_include_text=False):

        self.ds = ds


        #print("Number of rows = {0}".format(len(self.ds)))
        self.tokenizer = get_tokenizer(None)
        self.counter = Counter()
        self.vocab_hex = None
        self.optimal_sentence_length = None
        self.should_include_text = should_include_text

        if should_include_text:
            self.columns = ['masked_sentence', self.MASKED_INDICES_COLUMN, 'sentence', self.TARGET_COLUMN,
                            self.TOKEN_MASK_COLUMN]
        else:
            self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN, self.TOKEN_MASK_COLUMN]

        self.df = self.prepare_dataset()

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        item = self.df.iloc[idx]
        ds_item = self.ds.iloc[idx]
        # print(ds_item)

        inp = torch.Tensor(item[self.MASKED_INDICES_COLUMN]).long()
        token_mask = torch.Tensor(item[self.TOKEN_MASK_COLUMN]).bool()
        mask_target = torch.Tensor(item[self.TARGET_COLUMN]).long()
        mask_target = mask_target.masked_fill_(token_mask, 0 )


        attention_mask = (inp == self.vocab_hex[self.PAD]).unsqueeze(0)

        return (
            inp.to(device),
            attention_mask.to(device),
            token_mask.to(device),
            mask_target.to(device)

        )

    def prepare_dataset(self) -> pd.DataFrame:
        exp = []
        exp_lens = []

        for index, row in self.ds.iterrows():
            self.counter.update(row['higher_order_trajectory'].split())
            exp.append(row['higher_order_trajectory'])
            exp_lens.append(len(row['higher_order_trajectory'].split()))
        self.optimal_sentence_length = self._find_optimal_sentence_length(exp_lens)
        print(f"Optimal length exp = {self.optimal_sentence_length}")
        print("Create vocabulary")

        self._fill_vocab()

        prepeared_ds = []
        print("Preprocessing dataset")
        for i in tqdm(range(0,len(exp_lens))):
            proces_exp = self.tokenizer(exp[i])
            prepeared_ds.append(self._create_item(proces_exp))

        df = pd.DataFrame(prepeared_ds, columns=self.columns)
        self.findNeighboor()

        df[self.LABEL_COLUMN] = self.ds[self.LABEL_COLUMN]
        return df

    def _update_length(self, sentences: typing.List[str], lengths: typing.List[int]):
       # for v in sentences:
       #     l = len(v.split())
        lengths.append(len(sentences))
        return lengths

    def _find_optimal_sentence_length(self, lengths: typing.List[int]):
        arr = np.array(lengths)
        return int(np.percentile(arr, self.OPTIMAL_LENGTH_PERCENTILE))

    def _fill_vocab(self):
        # specials= argument is only in 0.12.0 version
        # specials=[self.CLS, self.PAD, self.MASK, self.SEP, self.UNK]
        self.vocab_hex = vocab(self.counter, min_freq=1)
        self.vocab_hex.insert_token(self.CLS, 0)
        self.vocab_hex.insert_token(self.PAD, 1)
        self.vocab_hex.insert_token(self.MASK, 2)
        self.vocab_hex.set_default_index(3)
        #self.vocab_hex = len(self.vocab_hex)
    def findNeighboor(self):
      self.ds["splited"] = self.ds["higher_order_trajectory"].str.split()

      uniqueTraject = []
      for row in self.ds["splited"]:
        uniqueTraject.extend(row)
      self.uniqueTraject = list(set(uniqueTraject))

      self.neighboorDict = {}
      for traj in uniqueTraject:
          self.neighboorDict[traj] = self.get_neighbors(traj, 1)


      self.pairEdges = []
      self.pairEdgesMapped = []

      for key,values in self.neighboorDict.items():
        for value in values:
          self.pairEdges.append([key,value])
          if value not in self.vocab_hex:
            self.vocab_hex.insert_token(value,len(self.vocab_hex))

      for origin,neigh in self.pairEdges:
        self.pairEdgesMapped.append([self.vocab_hex.lookup_indices([origin])[0],self.vocab_hex.lookup_indices([neigh])[0]])

      self.allNeigh = []
      for key,values in self.neighboorDict.items():
        self.allNeigh.append(key)
        for value in values:
          self.allNeigh.append(value)
      self.allNeigh = list(set(self.allNeigh))

    def get_neighbors(self, h3_id, degree):
        if degree == 0:
            return [h3_id]
        else:
            neighbors = h3.grid_ring(h3_id)
            result = []
            for neighbor in neighbors:
                result.extend(self.get_neighbors(neighbor, degree - 1))
            return result

    def _create_item(self, first: typing.List[str]):
        # Create masked sentence item
        updated_first, first_mask = self._preprocess_sentence(first.copy())
        #updated_second, second_mask = self._preprocess_sentence(second.copy())

        nsp_sentence = updated_first
        nsp_indices = self.vocab_hex.lookup_indices(nsp_sentence)
        inverse_token_mask = first_mask

        # Create sentence item without masking random words
        first, _ = self._preprocess_sentence(first.copy(), should_mask=False)
        #second, _ = self._preprocess_sentence(second.copy(), should_mask=False)
        original_nsp_sentence = first
        original_nsp_indices = self.vocab_hex.lookup_indices(original_nsp_sentence)

        if self.should_include_text:
            return (
                nsp_sentence,
                nsp_indices,
                original_nsp_sentence,
                original_nsp_indices,
                inverse_token_mask,
            )
        else:
            return (
                nsp_indices,
                original_nsp_indices,
                inverse_token_mask,
            )


    def _preprocess_sentence(self, sentence: typing.List[str], should_mask: bool = True):
        inverse_token_mask = None
        if should_mask:
            sentence, inverse_token_mask = self._mask_sentence(sentence)
        sentence, inverse_token_mask = self._pad_sentence([self.CLS] + sentence, inverse_token_mask)

        return sentence, inverse_token_mask

    def _mask_sentence(self, sentence: typing.List[str]):
        """Replace MASK_PERCENTAGE (15%) of words with special [MASK] symbol
        or with random word from vocabulary
        Args:
            sentence: sentence to process
        Returns:
            tuple of processed sentence and inverse token mask
        """
        len_s = len(sentence)
        #print(sentence)
        inverse_token_mask = [True for _ in range(max(len_s, self.optimal_sentence_length))]

        mask_amount = round(len_s * self.MASK_PERCENTAGE)
        for _ in range(mask_amount):
            i = random.randint(0, len_s - 1)

            if random.random() < 0.8:
                sentence[i] = self.MASK
            else:
                j = random.randint(0, len(self.vocab_hex) - 6)
                sentence[i] = self.vocab_hex.lookup_token(j)
            inverse_token_mask[i] = False
        return sentence, inverse_token_mask

    def _pad_sentence(self, sentence: typing.List[str], inverse_token_mask: typing.List[bool] = None):
        len_s = len(sentence)

        if len_s >= self.optimal_sentence_length:
            s = sentence[:self.optimal_sentence_length]
        else:
            s = sentence + [self.PAD] * (self.optimal_sentence_length - len_s)

        # inverse token mask should be padded as well

        if inverse_token_mask:
            inverse_token_mask = [True] + inverse_token_mask
            #print(len(sentence)==len(inverse_token_mask))
            len_m = len(inverse_token_mask)
            if len_m >= self.optimal_sentence_length:
                inverse_token_mask = inverse_token_mask[:self.optimal_sentence_length]
            else:
                inverse_token_mask = inverse_token_mask + [True] * (self.optimal_sentence_length - len_m-1)
        return s, inverse_token_mask

import torch
import gensim
from torch import nn
import torch.nn.functional as f
import math
import numpy as np
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from torch.nn.parameter import Parameter
from torch.nn.modules.module import Module
import torch as th
import math


class GCN_Layer(Module):
    def __init__(self,in_feature,out_feature,bias = True):
        super(GCN_Layer,self).__init__()

        self.in_feature = in_feature
        self.out_feature = out_feature
        self.weight = Parameter(th.FloatTensor(self.in_feature,self.out_feature))
        if bias:
            self.bias = Parameter(th.FloatTensor(self.out_feature))

        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self,X,A):
        # print("weight : {}".format(self.weight.shape))
        support = th.spmm(X, self.weight)
        # print("support : {}".format(support.shape))
        output = th.spmm(A, support)
        if self.bias is not None:
            return output + self.bias
        else:
            return output



class GCN(Module):
    def __init__(self, nfeature, nhiden, dropout):
        super(GCN, self).__init__()
        self.gc1 = GCN_Layer(nfeature, nhiden)
        # self.gc2 = GCN_Layer(nhiden, nhiden)
        self.dropout = dropout

    def forward(self, x, adj):
        x = self.gc1(x, adj)
        x = th.relu(x)
        x = th.dropout(x, self.dropout, train=self.training)
        return x

class JointEmbedding(nn.Module):

    def __init__(self, vocab_size, emb_dim):
        super(JointEmbedding, self).__init__()

        self.size = emb_dim

        self.emb =  nn.Embedding(vocab_size, emb_dim)

        self.norm = nn.LayerNorm(emb_dim)

    def forward(self, input_tensor):
        #sentence_size = input_tensor.size(-1)
        pos_tensor = self.attention_position(self.size, input_tensor)

        #segment_tensor = torch.zeros_like(input_tensor).to(device)
        #segment_tensor[:, sentence_size // 2 + 1:] = 1
        input_emb = self.emb(input_tensor)
        #output = pos_tensor + time_emb + self.s_emb(input_tensor)

        #( self.norm(pos_tensor),self.norm(time_emb),self.norm(self.s_emb(input_tensor)),self.norm(self.poi_emb(poi_input)) ) , self.norm(self.spatial_emb(input_tensor))
        return (self.norm(pos_tensor + input_emb ) )

    def attention_position(self, dim, input_tensor):
        batch_size = input_tensor.size(0)
        sentence_size = input_tensor.size(-1)

        pos = torch.arange(sentence_size, dtype=torch.long).to(device)
        d = torch.arange(dim, dtype=torch.long).to(device)
        d = (2 * d / dim)

        pos = pos.unsqueeze(1)
        pos = pos / (1e4 ** d)

        pos[:, ::2] = torch.sin(pos[:, ::2])
        pos[:, 1::2] = torch.cos(pos[:, 1::2])

        return pos.expand(batch_size, *pos.size())


class AttentionHead(nn.Module):

    def __init__(self, dim_inp, dim_out):
        super(AttentionHead, self).__init__()

        self.dim_inp = dim_inp
        self.q = nn.Linear(dim_inp, dim_out)
        self.k = nn.Linear(dim_inp, dim_out)
        self.v = nn.Linear(dim_inp, dim_out)
    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor = None):

        query, key, value = self.q(input_tensor), self.k(input_tensor), self.v(input_tensor)

        scale = query.size(1) ** 0.5
        scores = torch.bmm(query, key.transpose(1, 2)) / scale

        scores = scores.masked_fill_(attention_mask, -1e9)
        attn = f.softmax(scores, dim=-1)
        context = torch.bmm(attn, value)

        return context


class MultiHeadAttention(nn.Module):

    def __init__(self, num_heads, dim_inp, dim_out):
        super(MultiHeadAttention, self).__init__()

        self.heads = nn.ModuleList([
            AttentionHead(dim_inp, dim_out) for _ in range(num_heads)
        ])
        self.linear = nn.Linear(dim_out * num_heads, dim_inp)
        self.norm = nn.LayerNorm(dim_inp)

    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):
        s = [head(input_tensor, attention_mask) for head in self.heads]
        scores = torch.cat(s, dim=-1)
        scores = self.linear(scores)
        return self.norm(scores)


class Encoder(nn.Module):

    def __init__(self, dim_inp, dim_out, attention_heads=4, dropout=0.1):
        super(Encoder, self).__init__()

        self.attention = MultiHeadAttention(attention_heads, dim_inp, dim_out)  # batch_size x sentence size x dim_inp
        self.feed_forward = nn.Sequential(
            nn.Linear(dim_inp, dim_out),
            nn.Dropout(dropout),
            nn.GELU(),
            nn.Linear(dim_out, dim_inp),
            nn.Dropout(dropout)
        )
        self.norm = nn.LayerNorm(dim_inp)

    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):
        context = self.attention(input_tensor, attention_mask)
        res = self.feed_forward(context)
        return self.norm(res)


class BERT_SIMPLE(nn.Module):

    def __init__(self, vocab_size, dim_inp, dim_out, attention_heads=4, user_size=1, mode=0):
        super(BERT_SIMPLE, self).__init__()

        #mode 1 TUL, mode 0 for Token prediction training
        # self.embedding = JointEmbedding(vocab_size, dim_inp,)
        self.embedding =  nn.Embedding(vocab_size, dim_inp)
        self.conv = GCN(dim_inp, dim_inp,0.5)
        self.encoder = Encoder(dim_inp, dim_out, attention_heads)
        # self.encoder1 = Encoder(dim_inp, dim_out, attention_heads)
        self.token_prediction_layer = nn.Linear(dim_inp, vocab_size)
        self.softmax = nn.LogSoftmax(dim=-1)
        self.user_classification_layer = nn.Linear(dim_inp, user_size)
        self.mode = mode

    def forward(self, input_tensor, attention_mask,x,adj):
        #print("reach here")

        if self.mode == 0:
            comb_embd = self.embedding(x)
            comb_embd = self.conv(comb_embd, adj)
            comb_embd = comb_embd[input_tensor]
            encoded = self.encoder(comb_embd, attention_mask)
            token_predictions = self.token_prediction_layer(encoded)
            
            return self.softmax(token_predictions)
        else:
            comb_embd = self.embedding(input_tensor)
            encoded = self.encoder(comb_embd, attention_mask)
            user_classifications = self.user_classification_layer(encoded[:, 0])
            return user_classifications




    # def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):
    #     #print("reach here")
    #     comb_embd = self.embedding(input_tensor)
    #     encoded = self.encoder(comb_embd, attention_mask)
    #     encoded1 = self.encoder(encoded, attention_mask)

    #     if self.mode == 0:
    #       token_predictions = self.token_prediction_layer(encoded)
    #       return self.softmax(token_predictions)
    #     else:
    #       user_classifications = self.user_classification_layer(encoded[:, 0])
    #       return user_classifications
    def change_mode (self, new_mode):
      if new_mode != 0  and new_mode != 1:
        raise ValueError(f"{new_mode} is an invalid mode, choose 0 for training or 1 for evaluating")
      else:
        self.mode = new_mode

import time
from datetime import datetime
from pathlib import Path

import torch

from torch import nn
from torch.utils.data import DataLoader

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#device = torch.device("cpu")

def percentage(batch_size: int, max_index: int, current_index: int):
    """Calculate epoch progress percentage
    Args:
        batch_size: batch size
        max_index: max index in epoch
        current_index: current index
    Returns:
        Passed percentage of dataset
    """
    batched_max = max_index // batch_size
    return round(current_index / batched_max * 100, 2)


def nsp_accuracy(result: torch.Tensor, target: torch.Tensor):
    """Calculate NSP accuracy between two tensors
    Args:
        result: result calculated by model
        target: real target
    Returns:
        NSP accuracy
    """
    s = (result.argmax(1) == target.argmax(1)).sum()
    return round(float(s / result.size(0)), 2)


def token_accuracy(result: torch.Tensor, target: torch.Tensor, inverse_token_mask: torch.Tensor):
    """Calculate MLM accuracy between ONLY masked words
    Args:
        result: result calculated by model
        target: real target
        inverse_token_mask: well-known inverse token mask
    Returns:
        MLM accuracy
    """
    r = result.argmax(-1).masked_select(~inverse_token_mask)
    print(r.size())
    #print(result.size())
    t = target.masked_select(~inverse_token_mask)
    s = (r == t).sum()
    print("SUM = {0}".format(s))
    print("result.size(0) = {0}".format(result.size(0) ))
    print("result.size(1) = {0}".format(result.size(1)))
    return float(s / r.size(0))


class BertTrainer:

    def __init__(self,
                 model: BERT_SIMPLE,
                 dataset: BERT_SIMPLE_PREP,
                 log_dir: Path,
                 checkpoint_dir: Path = None,
                 print_progress_every: int = 10,
                 print_accuracy_every: int = 50,
                 batch_size: int = 24,
                 learning_rate: float = 0.005,
                 epochs: int = 5
                 ):
        self.model = model
        self.dataset = dataset

        self.batch_size = batch_size
        self.epochs = epochs
        self.current_epoch = 0

        self.loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)

        #self.writer = SummaryWriter(str(log_dir))
        self.checkpoint_dir = checkpoint_dir

        self.ml_criterion = nn.NLLLoss(ignore_index=0).to(device)
        #self.ml_criterion_spatial = nn.MSELoss(reduction = 'sum').to(device)
        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        #self.optimizer1 = torch.optim.Adam(multimodel.parameters(), lr=learning_rate)
        #self.multimodelTask = multimodel
        #, weight_decay=0.05
        self._splitter_size = 35

        self._ds_len = len(self.dataset)
        self._batched_len = self._ds_len // self.batch_size

        self._print_every = print_progress_every
        self._accuracy_every = print_accuracy_every

    def print_summary(self):
        ds_len = len(self.dataset)

        print("Model Summary\n")
        print('=' * self._splitter_size)
        print(f"Device: {device}")
        print(f"Training dataset len: {ds_len}")
        print(f"Max / Optimal sentence len: {self.dataset.optimal_sentence_length}")
        print(f"Vocab size: {len(self.dataset.vocab_hex)}")
        print(f"Batch size: {self.batch_size}")
        print(f"Batched dataset len: {self._batched_len}")
        print('=' * self._splitter_size)
        print()

    def __call__(self):
        for self.current_epoch in range(self.current_epoch, self.epochs):
            #print(torch.cuda.memory_allocated(0)/1024/1024/1024)
            loss = self.train(self.current_epoch)
            self.save_checkpoint(self.current_epoch, step=-1, loss=0)
            #print(torch.cuda.memory_allocated(device))


    def train(self, epoch: int):
        print(f"Begin epoch {epoch}")
        prev = time.time()
        average_mlm_loss = 0
        for i, value in enumerate(self.loader):
            #print(value)
            index = i + 1
            inp, mask, inverse_token_mask, token_target = value
            self.optimizer.zero_grad()

            # token = self.model(inp, mask)
            token = self.model(inp, mask,X,adj) 

            tm = inverse_token_mask.unsqueeze(-1).expand_as(token)

            token = token.masked_fill(tm, 0)

            loss_token = self.ml_criterion(token.transpose(1, 2), token_target)

            loss_token.backward()
            self.optimizer.step()  # apply gradient step

            average_mlm_loss += loss_token.item()


            if index % self._print_every == 0:
                elapsed = time.gmtime(time.time() - prev)
                s = self.training_summary(elapsed, index, average_mlm_loss)
                if index % self._accuracy_every == 0:
                    s += self.accuracy_summary(index, token, token_target, inverse_token_mask)
                print(s)

                average_mlm_loss = 0
        return loss_token

    def training_summary(self, elapsed, index, average_mlm_loss):
        passed = percentage(self.batch_size, self._ds_len, index)
        global_step = self.current_epoch * len(self.loader) + index

        print_mlm_loss = average_mlm_loss / self._print_every

        s = f"{time.strftime('%H:%M:%S', elapsed)}"
        s += f" | Epoch {self.current_epoch + 1} | {index} / {self._batched_len} ({passed}%) | " \
              f"MLM loss {print_mlm_loss:6.2f}"


        return s

    def accuracy_summary(self, index, token, token_target, inverse_token_mask):
        global_step = self.current_epoch * len(self.loader) + index
        token_acc = token_accuracy(token, token_target, inverse_token_mask)


        return f" | Token accuracy {token_acc}"

    def save_checkpoint(self, epoch, step, loss):
        if not self.checkpoint_dir:
            return

        prev = time.time()
        name = f"bert_epoch{epoch}_step{step}_{datetime.utcnow().timestamp():.0f}.pt"

        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict()
        }, self.checkpoint_dir)

        print()
        print('=' * self._splitter_size)
        print(f"Model saved as '{name}' for {time.time() - prev:.2f}s")
        print('=' * self._splitter_size)
        print()

    def load_checkpoint(self, path: Path):
        print('=' * self._splitter_size)
        print(f"Restoring model {path}")
        checkpoint = torch.load(path)
        self.current_epoch = checkpoint['epoch']
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print("Model is restored.")
        print('=' * self._splitter_size)

import random
import typing
from collections import Counter
from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd
import torch
import pickle
from tqdm import tqdm
from torch.utils.data import Dataset
from torchtext.vocab import vocab
from torchtext.data.utils import get_tokenizer
from gensim.models import Word2Vec
import h3
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class BERT_SIMPLE_TUL_PREP(Dataset):
    CLS = '[CLS]'
    PAD = '[PAD]'
    MASK = '[MASK]'

    HEX_INDICES_COLUMN = 'hex_indices'
    USER_ID_COLUMN ='User_id'
    OPTIMAL_LENGTH_PERCENTILE = 100


    def __init__(self, ds, hex_vocab, user_vocab=None):

        self.ds =  ds

        self.tokenizer = get_tokenizer(None)
        self.user_counter = Counter()
        self.vocab = hex_vocab
        self.optimal_sentence_length = None
        self.columns = [self.HEX_INDICES_COLUMN, self.USER_ID_COLUMN]
        self.user_vocab = user_vocab
        self.df = self.prepare_dataset()


    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        item = self.df.iloc[idx]
        inp = torch.Tensor(item[self.HEX_INDICES_COLUMN]).long()
        attention_mask = (inp == self.vocab[self.PAD]).unsqueeze(0)
        user_item=torch.Tensor([item[self.USER_ID_COLUMN]]).long()
        return (
            inp.to(device),
            attention_mask.to(device),
            user_item.to(device),
        )

    def prepare_dataset(self) -> pd.DataFrame:
        exp = []
        exp_lens = []
        user_ids = []

        for index, row in self.ds.iterrows():
            self.user_counter.update([str(row['user_id'])])
            exp.append(row['higher_order_trajectory'])
            user_ids.append(str(row['user_id']))
            exp_lens.append(len(row['higher_order_trajectory'].split()))
        self.optimal_sentence_length = self._find_optimal_sentence_length(exp_lens, self.OPTIMAL_LENGTH_PERCENTILE)
        print(f"Optimal length exp = {self.optimal_sentence_length}")
        print("Create vocabulary")
        if self.user_vocab == None:
          self._fill_user_vocab()
        prepared_ds = []
        print("Preprocessing dataset")
        for i in tqdm(range(0,len(exp))):
            input = self.tokenizer(exp[i])
            prepared_ds.append(self._create_item(input, user_ids[i]))

        df = pd.DataFrame(prepared_ds, columns=self.columns)
        return df


    def _find_optimal_sentence_length(self, lengths: typing.List[int], perce):
        arr = np.array(lengths)
        return int(np.percentile(arr, perce))
    def _fill_user_vocab(self):
        self.user_vocab = vocab(self.user_counter, min_freq=1)

    def _create_item(self, sentence_inp: typing.List[str], user_id:int):
        updated_sentence_inp = self._preprocess_sentence(sentence_inp.copy())
        token_indices = self.vocab.lookup_indices(updated_sentence_inp)
        user_id = self.user_vocab.lookup_indices([user_id])[0]

        return (token_indices,user_id)

    def _preprocess_sentence(self, sentence, should_mask: bool = True):
        sentence = self._pad_sentence([self.CLS] + sentence)
        return sentence


    def _pad_sentence(self, sentence):
        len_s = len(sentence)

        if len_s >= self.optimal_sentence_length:
            s = sentence[:self.optimal_sentence_length]
        else:
            s = sentence + [self.PAD] * (self.optimal_sentence_length - len_s)

        return s

    def _preprocess_sentence(self, sentence, should_mask: bool = True):
        sentence = self._pad_sentence([self.CLS] + sentence)
        return sentence
    def class_weight(self):
        result = []
        for i in self:
          result.append(i[2].item())
        return [ x[1] for x in sorted(Counter(result).items()) ]

# if __name__ == '__main__':
#     result = BERT_SIMPLE_PREP("/content/drive/MyDrive/Baselines/ST-BERT/dataset/ST-BERT NYC/hex7/nyc_hex7_test_209_users.csv")
#     result = BERT_SIMPLE_TUL_PREP("/content/drive/MyDrive/Baselines/ST-BERT/dataset/ST-BERT NYC/hex7/nyc_hex7_test_209_users.csv",result.vocab_hex,None)

import time
from datetime import datetime
from pathlib import Path

import torch
from sklearn.metrics import f1_score
from torch import nn
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import StepLR
from balanced_loss import Loss
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



class BertTrainerclassification:

    def __init__(self,
                 model: BERT_SIMPLE,
                 dataset: BERT_SIMPLE_TUL_PREP,
                 testdataset: BERT_SIMPLE_TUL_PREP,
                 log_dir: Path,
                 checkpoint_dir: Path = None,
                 print_progress_every: int = 10,
                 print_accuracy_every: int = 50,
                 batch_size: int = 24,
                 learning_rate: float = 0.005,
                 epochs: int = 5,
                 ):
        self.model = model
        self.dataset = dataset
        self.ds_test = testdataset
        self.batch_size = batch_size
        self.epochs = epochs
        self.current_epoch = 0

        self.loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)
        self.test_loader = DataLoader(self.ds_test, batch_size=1 , shuffle=True)

        #self.writer = SummaryWriter(str(log_dir))
        self.checkpoint_dir = checkpoint_dir
        self.ml_criterion = Loss(loss_type="cross_entropy",samples_per_class=self.dataset.class_weight(),class_balanced=True)
        #self.ml_criterion = nn.CrossEntropyLoss().to(device)
        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[40,50,60], gamma=0.5)
        #self.optimizer1 = torch.optim.Adam(multimodel.parameters(), lr=learning_rate)

        self._splitter_size = 35

        self._ds_len = len(self.dataset)
        self._batched_len = self._ds_len // self.batch_size

        self._print_every = print_progress_every
        self._accuracy_every = print_accuracy_every

    def print_summary(self):
        ds_len = len(self.dataset)

        print("Model Summary\n")
        print('=' * self._splitter_size)
        print(f"Device: {device}")
        print(f"Training dataset len: {ds_len}")
        print(f"Max / Optimal sentence len: {self.dataset.optimal_sentence_length}")
        print(f"Vocab size: {len(self.dataset.vocab)}")
        print(f"Batch size: {self.batch_size}")
        print(f"Batched dataset len: {self._batched_len}")
        print('=' * self._splitter_size)
        print()

    def __call__(self):
        for self.current_epoch in range(self.current_epoch, self.epochs):
            #print(torch.cuda.memory_allocated(0)/1024/1024/1024)
            loss = self.train(self.current_epoch)

            #self.save_checkpoint(self.current_epoch, step=-1, loss=loss)
            #print(torch.cuda.memory_allocated(device))
            self.calculate_test_accuracy()

    def calculate_test_accuracy(self):
        self.model.eval()
        top1 = 0
        top3 = 0
        top5 = 0
        top7 = 0
        top10 = 0
        targets = []
        results = []
        with torch.no_grad():
          for i , value in enumerate(self.test_loader):
            input,mask,target= value
            targets.append(target.item())
          #print(input.size())
          #print(mask.size())
          #print(target.size())
            #print(input)
           # print(mask)
           # print(poi)
            # result = self.model(input,mask)
            result = self.model(input, mask,X,adj)
            top1 += self.find_k_accuracy(1,result,target)
            results.append(torch.topk(result, 1).indices.item())
            top3 += self.find_k_accuracy(3,result,target)
            top5 += self.find_k_accuracy(5,result,target)
            top7 += self.find_k_accuracy(7,result,target)
            top10 += self.find_k_accuracy(10,result,target)
            #print(targets)
            #print(results)
        top1 = top1/len(self.ds_test)
        top3 = top3/len(self.ds_test)
        top5 = top5/len(self.ds_test)
        top7 = top7/len(self.ds_test)
        top10 = top10/len(self.ds_test)
        f1_score_macro = f1_score(targets, results, average='macro')
        r_score = recall_score(targets, results, average='macro')
        p_score = precision_score(targets, results, average='macro')

        txt = f"top@1 = {top1} | top@3 = {top3} | top@5 = {top5} | macro-P={p_score} | macro-R{r_score} |f1_score_macro={f1_score_macro}"
        print(txt)
        with open("/home/tlmkhoa/gcnTUL/tulGCN_results/{}.txt".format(file), "w") as text_file:
            text_file.write(txt)
        self.model.train()



    def find_k_accuracy(self,k,result,target):
      #print(result)
      #print(target)
      if target in torch.topk(result, k).indices:
        return 1
      return 0

    def train(self, epoch: int):
        print(f"Begin epoch {epoch}")
        prev = time.time()
        average_class_loss = 0
        #print(torch.cuda.memory_allocated(0)/1024/1024/1024)
        for i, value in enumerate(self.loader):
            self.optimizer.zero_grad()
            index = i + 1
            inp, mask, target= value

            #print(inp.size())
            #print(mask.size())
            # result = self.model(inp,mask)
            result =  self.model(inp, mask,X,adj)
            #result = self.model(inp, mask)
            #print(target.size())
            #print(result.size())


            loss = self.ml_criterion(result, torch.flatten(target))
            loss.backward()
            self.optimizer.step()

            average_class_loss += +loss.item()

            #self.optimizer1.step()
            #break

            if index % self._print_every == 0:
                print( f" average_class_loss {average_class_loss / self._print_every}" )
                average_class_loss = 0
        self.scheduler.step()
        return loss

import h3
import networkx as nx
from datetime import datetime
import torch
from pathlib import Path
import gc
import sys
EMB_SIZE = 512
HIDDEN_SIZE = 512
EPOCHS = 1
BATCH_SIZE = 12
NUM_HEADS = 16

timestamp = datetime.utcnow().timestamp()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
gc.collect()

if torch.cuda.is_available():
    torch.cuda.empty_cache()

if __name__ == '__main__':
    print("Prepare dataset")
    gc.collect()
    torch.cuda.empty_cache()

    from sklearn.model_selection import train_test_split
    file = "ho_tdrive_res6"
    df_full_file = pd.read_csv("/home/tlmkhoa/gcnTUL/higher_order_trajectory/tdrive/{}.csv".format(file)).rename(columns={"taxi_id":"user_id"})
    print(df_full_file.columns)
    print(device)
    remainUserList = df_full_file["user_id"].value_counts()[df_full_file["user_id"].value_counts().values >  2].index
    df_full_file = df_full_file[df_full_file["user_id"].isin(remainUserList)]
    df_full_file = df_full_file.reset_index(drop=True)
        
    max_length = 300
    def get_from_max_length(lst):
        return lst[:max_length]
    
    df_full_file["higher_order_trajectory"] = df_full_file["higher_order_trajectory"].str.split().apply(get_from_max_length).str.join(sep = " ")

    X_train, X_test, y_train, y_test = train_test_split(df_full_file.drop(["user_id"],axis =1 ), df_full_file["user_id"], test_size=0.33, random_state=42,stratify = df_full_file["user_id"])

    X_train["user_id"] = y_train
    X_test["user_id"] = y_test

    ds = BERT_SIMPLE_PREP(df_full_file)
    ds_full = BERT_SIMPLE_TUL_PREP(df_full_file, hex_vocab= ds.vocab_hex)
    ds_test = BERT_SIMPLE_TUL_PREP(X_test,hex_vocab= ds.vocab_hex,user_vocab=ds_full.user_vocab)
    ds_train = BERT_SIMPLE_TUL_PREP(X_train,hex_vocab= ds.vocab_hex,user_vocab=ds_full.user_vocab)



    # --------------------------------------------------------------------------------------------------------------------------
    ds.df["Original_seq_length"] = df_full_file.splited.apply(len)
    # message passing use trajactory
    originalSeq = ds.df[ds.df["Original_seq_length"] >= 2]["indices"]

    from collections import defaultdict
    def count_pairs(list_of_lists):
        # Initialize a dictionary to store the counts
        pair_counts = defaultdict(int)

        # Loop over each list in the list of lists
        for lst in list_of_lists:
            # Loop over each pair of elements in the list
            for i in range(len(lst) - 1):
                # Create a tuple for the pair, sorted so order doesn't matter
                pair = tuple(sorted((lst[i], lst[i+1])))
                # Increment the count for this pair
                pair_counts[pair] += 1

        return pair_counts

    seqCountDict = count_pairs(originalSeq)

    
    graph = nx.Graph()
    for value in range(len(ds.vocab_hex)):
        graph.add_node(value)

    # graph.add_node(value+1)
    graph.add_edges_from(ds.pairEdgesMapped, weight = 1)

    for (node1,node2), weight in seqCountDict.items():
        graph.add_edge(node1,node2, weight = weight)

    adj = nx.adjacency_matrix(graph)
    adj

    import scipy.sparse as sp
    import numpy as np
    def normalizeAdj(adj):
        print("Normalizing Adjacency matrix ...")
        # since adj is undirected graph -> this operation make sure its  is symmetric
        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)

        # add in identity rows, self connection
        adj = adj + sp.eye(adj.shape[0])

        # create D^-1/2
        rowsum = np.array(adj.sum(1))
        d_inv_sqrt = np.power(rowsum, -0.5).flatten()
        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)

        # apply normalize
        A_hat = d_mat_inv_sqrt@adj@d_mat_inv_sqrt

        return A_hat

    adj = normalizeAdj(adj)

    import torch
    def scipyToTorchSparse(scipySparse):
        print("Converting to Torch Sparse ...")
        scipySparse = sp.coo_matrix(scipySparse)
        values = scipySparse.data
        indices = np.vstack((scipySparse.row, scipySparse.col))

        i = torch.LongTensor(indices)
        v = torch.FloatTensor(values)
        shape = scipySparse.shape

        TorchSparse = torch.sparse.FloatTensor(i, v, torch.Size(shape))

        return  TorchSparse

    adj = scipyToTorchSparse(adj)

    # ------------------------------------------------------------------------

    X = torch.tensor([i for i in range(len(ds.vocab_hex))]).to(device)
    adj = adj.to(device)

    # -------------------------------------------------------------

    bert = BERT_SIMPLE(len(ds.vocab_hex), EMB_SIZE, HIDDEN_SIZE, NUM_HEADS, user_size=len(ds_full.user_vocab), mode=0 ).to(device)
    trainer = BertTrainer(
        model=bert,
        dataset=ds,
        log_dir=None,
        checkpoint_dir="",
        print_progress_every=5,
        print_accuracy_every=5,
        batch_size=BATCH_SIZE,
        learning_rate=0.00001,
        epochs=EPOCHS,
    )
    trainer.print_summary()
    trainer()

ds_full = BERT_SIMPLE_TUL_PREP(df_full_file, hex_vocab= ds.vocab_hex)
ds_test = BERT_SIMPLE_TUL_PREP(X_test,hex_vocab= ds.vocab_hex,user_vocab=ds_full.user_vocab)
ds_train = BERT_SIMPLE_TUL_PREP(X_train,hex_vocab= ds.vocab_hex,user_vocab=ds_full.user_vocab)
bert.change_mode(1)
print("==============")
print(len(ds_full.user_vocab))
ds_train.class_weight()
trainer_classifications = BertTrainerclassification(
        model=bert,
        dataset=ds_train,
        testdataset=ds_test,
        log_dir="None",
        checkpoint_dir="None",
        print_progress_every=50,
        print_accuracy_every=50,
        batch_size=BATCH_SIZE,
        learning_rate=0.00002,
        epochs=70,

    )
trainer_classifications.print_summary()
trainer_classifications()

